{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import urllib\n",
    "import os\n",
    "from pathlib import Path\n",
    "import email\n",
    "import email.policy\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_data():\n",
    "    root_url=\"http://spamassassin.apache.org/old/publiccorpus/\"\n",
    "    ham_url=root_url+\"20030228_easy_ham.tar.bz2\"\n",
    "    spam_url=root_url+\"20030228_spam.tar.bz2\"\n",
    "\n",
    "    spam_path=Path()/\"data\"/\"spam\"\n",
    "    spam_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for dir_name, tar_name, url in ((\"easy_ham\", \"ham\", ham_url), \n",
    "                                    (\"spam\", \"spam\", spam_url)):\n",
    "        if not (spam_path/dir_name).is_dir():\n",
    "            path=(spam_path/tar_name).with_suffix(\".tar.bz2\")\n",
    "            print(\"Downloading\", path)\n",
    "            urllib.request.urlretrieve(url, path)\n",
    "\n",
    "            tar_file=tarfile.open(path)\n",
    "            tar_file.extractall(path=spam_path)\n",
    "            tar_file.close()\n",
    "\n",
    "    return [spam_path/dir_name for dir_name in (\"easy_ham\", \"spam\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ham_dir, spam_dir=fetch_data()\n",
    "\n",
    "ham_filenames=[f for f in sorted(ham_dir.iterdir()) if len(f.name)>20]\n",
    "spam_filenames=[f for f in sorted(spam_dir.iterdir()) if len(f.name)>20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2500"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ham_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(spam_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_email(filepath):\n",
    "    with open(filepath, \"rb\") as file:\n",
    "        return email.parser.BytesParser(policy=email.policy.default).parse(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_emails=[load_email(filepath) for filepath in ham_filenames]\n",
    "spam_emails=[load_email(filepath) for filepath in spam_filenames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> I just had to jump in here as Carbonara is one of my favourites to make and \n",
      "> ask \n",
      "> what the hell are you supposed to use instead of cream? \n",
      "\n",
      "Isn't it just basically a mixture of beaten egg and bacon (or pancetta, \n",
      "really)? You mix in the raw egg to the cooked pasta and the heat of the pasta \n",
      "cooks the egg. That's my understanding.\n",
      "\n",
      "Martin\n",
      "\n",
      "------------------------ Yahoo! Groups Sponsor ---------------------~-->\n",
      "4 DVDs Free +s&p Join Now\n",
      "http://us.click.yahoo.com/pt6YBB/NXiEAA/mG3HAA/7gSolB/TM\n",
      "---------------------------------------------------------------------~->\n",
      "\n",
      "To unsubscribe from this group, send an email to:\n",
      "forteana-unsubscribe@egroups.com\n",
      "\n",
      " \n",
      "\n",
      "Your use of Yahoo! Groups is subject to http://docs.yahoo.com/info/terms/\n"
     ]
    }
   ],
   "source": [
    "print(ham_emails[5].get_content().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A POWERHOUSE GIFTING PROGRAM You Don't Want To Miss! \n",
      " \n",
      "  GET IN WITH THE FOUNDERS! \n",
      "The MAJOR PLAYERS are on This ONE\n",
      "For ONCE be where the PlayerS are\n",
      "This is YOUR Private Invitation\n",
      "\n",
      "EXPERTS ARE CALLING THIS THE FASTEST WAY \n",
      "TO HUGE CASH FLOW EVER CONCEIVED\n",
      "Leverage $1,000 into $50,000 Over and Over Again\n",
      "\n",
      "THE QUESTION HERE IS:\n",
      "YOU EITHER WANT TO BE WEALTHY \n",
      "OR YOU DON'T!!!\n",
      "WHICH ONE ARE YOU?\n",
      "I am tossing you a financial lifeline and for your sake I \n",
      "Hope you GRAB onto it and hold on tight For the Ride of youR life!\n",
      "\n",
      "Testimonials\n",
      "\n",
      "Hear what average people are doing their first few days:\n",
      "�We've received 8,000 in 1 day and we are doing that over and over again!' Q.S. in AL\n",
      " �I'm a single mother in FL and I've received 12,000 in the last 4 days.� D. S. in FL\n",
      "�I was not sure about this when I sent off my $1,000 pledge, but I got back $2,000 the very next day!� L.L. in KY\n",
      "�I didn't have the money, so I found myself a partner to work this with. We have received $4,000 over the last 2 days. \n",
      "I think I made the right decision; don't you?� K. C. in FL\n",
      "�I pick up $3,000 my first day and I  they gave me free leads and all the training, you can too!� J.W. in CA\n",
      "\n",
      "ANNOUNCING: We will CLOSE your sales for YOU! And Help you get a Fax Blast IMMEDIATELY Upon Your Entry!!!    YOU Make the MONEY!!!\n",
      "FREE LEADS!!! TRAINING!!!\n",
      "\n",
      "$$DON'T WAIT!!! CALL NOW $$\n",
      "FAX BACK TO: 1-800-421-6318 OR Call 1-800-896-6568 \n",
      "\n",
      "Name__________________________________Phone___________________________________________\n",
      "\n",
      "Fax_____________________________________Email____________________________________________\n",
      "\n",
      "Best Time To Call_________________________Time Zone________________________________________\n",
      "\n",
      "This message is sent in compliance of the new e-mail bill. \"Per Section 301, Paragraph (a)(2)(C) of S. 1618, further transmissions by the sender of this email may be stopped, at no cost to you, by sending a reply to this email address with the word \"REMOVE\" in the subject line. Errors, omissions, and exceptions excluded.\n",
      " \n",
      "This is NOT spam! I have compiled this list from our Replicate Database, relative to Seattle Marketing Group, The Gigt, or Turbo Team for the sole purpose of these communications. Your continued inclusion is ONLY by your gracious permission. If you wish to not receive this mail from me, please send an email to tesrewinter@yahoo.com with \"Remove\" in the subject and you will be deleted immediately.\n"
     ]
    }
   ],
   "source": [
    "print(spam_emails[5].get_content().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_email_stracture(email):\n",
    "    if isinstance(email, str):\n",
    "        return email\n",
    "    payload=email.get_payload()\n",
    "\n",
    "    if isinstance(payload, list):\n",
    "        multipart=', '.join([get_email_stracture(sub_email) for sub_email in payload])\n",
    "        return f'multipart({multipart})'\n",
    "    else:\n",
    "        return email.get_content_type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stractures_counter(emails):\n",
    "    stractures = Counter()\n",
    "    for email in emails:\n",
    "        starcture=get_email_stracture(email)\n",
    "        stractures[starcture] += 1\n",
    "    return stractures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('text/plain', 2408),\n",
       " ('multipart(text/plain, application/pgp-signature)', 66),\n",
       " ('multipart(text/plain, text/html)', 8),\n",
       " ('multipart(text/plain, text/plain)', 4),\n",
       " ('multipart(text/plain)', 3),\n",
       " ('multipart(text/plain, application/octet-stream)', 2),\n",
       " ('multipart(text/plain, text/enriched)', 1),\n",
       " ('multipart(text/plain, application/ms-tnef, text/plain)', 1),\n",
       " ('multipart(multipart(text/plain, text/plain, text/plain), application/pgp-signature)',\n",
       "  1),\n",
       " ('multipart(text/plain, video/mng)', 1),\n",
       " ('multipart(text/plain, multipart(text/plain))', 1),\n",
       " ('multipart(text/plain, application/x-pkcs7-signature)', 1),\n",
       " ('multipart(text/plain, multipart(text/plain, text/plain), text/rfc822-headers)',\n",
       "  1),\n",
       " ('multipart(text/plain, multipart(text/plain, text/plain), multipart(multipart(text/plain, application/x-pkcs7-signature)))',\n",
       "  1),\n",
       " ('multipart(text/plain, application/x-java-applet)', 1)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stractures_counter(ham_emails).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('text/plain', 218),\n",
       " ('text/html', 183),\n",
       " ('multipart(text/plain, text/html)', 45),\n",
       " ('multipart(text/html)', 20),\n",
       " ('multipart(text/plain)', 19),\n",
       " ('multipart(multipart(text/html))', 5),\n",
       " ('multipart(text/plain, image/jpeg)', 3),\n",
       " ('multipart(text/html, application/octet-stream)', 2),\n",
       " ('multipart(text/plain, application/octet-stream)', 1),\n",
       " ('multipart(text/html, text/plain)', 1),\n",
       " ('multipart(multipart(text/html), application/octet-stream, image/jpeg)', 1),\n",
       " ('multipart(multipart(text/plain, text/html), image/gif)', 1),\n",
       " ('multipart/alternative', 1)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stractures_counter(spam_emails).most_common()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import re\n",
    "from html import unescape\n",
    "\n",
    "import nltk\n",
    "import urlextract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.array(ham_emails+spam_emails, dtype=object)\n",
    "y=np.array([0]*len(ham_emails) + [1]*len(spam_emails))\n",
    "\n",
    "X_train, X_test, y_train, y_test=train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def html_to_text(html):\n",
    "    text=re.sub('<head.*?>.*?</head>', '', html, flags=re.M | re.S | re.I)\n",
    "    text=re.sub('<a\\s.*?>', 'HYPERLINK', text, flags=re.M | re.S | re.I)\n",
    "    text=re.sub('<.*?>', '', text, flags=re.M | re.S)\n",
    "    text=re.sub(r'(\\s*\\n)+', '\\n', text, flags=re.M | re.S)\n",
    "    return unescape(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def email_to_text(email):\n",
    "    html=None\n",
    "    for part in email.walk():\n",
    "        ctype=part.get_content_type()\n",
    "        if not ctype in ('text/plain', 'text/html'):\n",
    "            continue\n",
    "        try:\n",
    "            content=part.get_content()\n",
    "        except:\n",
    "            content=str(part.get_payload())\n",
    "        if ctype=='text/plain':\n",
    "            return content\n",
    "        else:\n",
    "            html=content\n",
    "    if html:\n",
    "        return html_to_text(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer=nltk.PorterStemmer()\n",
    "url_extractor=urlextract.URLExtract()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing PipeLine"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Custom Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmailToWordCounterTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, strip_header=True, lower_case=True, remove_punctuation=True,\n",
    "                 replace_urls=True, replace_numbers=True, stemming=True):\n",
    "        self.strip_header=strip_header\n",
    "        self.lower_case=lower_case\n",
    "        self.remove_punctuation=remove_punctuation\n",
    "        self.replace_urls=replace_urls\n",
    "        self.replace_numbers=replace_numbers\n",
    "        self.stemming=stemming\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X_transformed=[]\n",
    "        for email in X:\n",
    "            text =email_to_text(email) or ''\n",
    "\n",
    "            if self.lower_case:\n",
    "                text=text.lower()\n",
    "            if self.replace_urls and url_extractor is not None:\n",
    "                urls=list(set(url_extractor.find_urls(text)))\n",
    "                urls.sort(key=lambda url: len(url), reverse=True)\n",
    "                for url in urls:\n",
    "                    text=text.replace(url, \" URL \")\n",
    "            if self.replace_numbers:\n",
    "                text=re.sub(r'\\d+(?:\\.\\d*)?(?:[eE][+-]?\\d+)?', 'NUMBER', text)\n",
    "            if self.remove_punctuation:\n",
    "                text = re.sub(r'\\W+', ' ', text, flags=re.M)\n",
    "            word_counts=Counter(text.split())\n",
    "            if self.stemming and stemmer is not None:\n",
    "                stemmed_word_counts=Counter()\n",
    "                for word, count in word_counts.items():\n",
    "                    stemmed_word=stemmer.stem(word)\n",
    "                    stemmed_word_counts[stemmed_word] += count\n",
    "                word_counts=stemmed_word_counts\n",
    "            X_transformed.append(word_counts)\n",
    "        return np.array(X_transformed)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordCounterToVectorTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, vocabulary_size=1000):\n",
    "        self.vocabulary_size=vocabulary_size\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        total_count=Counter()\n",
    "        for word_count in X:\n",
    "            for word, count in word_count.items():\n",
    "                total_count[word] += min(count, 10)\n",
    "        most_common=total_count.most_common()[:self.vocabulary_size]\n",
    "        self.vocabulary_={word: index+1 for index, (word, count) in enumerate(most_common)}\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        rows=[]\n",
    "        columns=[]\n",
    "        data=[]\n",
    "        for row, word_count in enumerate(X):\n",
    "            for word, count in word_count.items():\n",
    "                rows.append(row)\n",
    "                columns.append(self.vocabulary_.get(word, 0))\n",
    "                data.append(count)\n",
    "        return csr_matrix((data, (rows, columns)), shape=(len(X), self.vocabulary_size+1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the PipeLine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_pipeline=Pipeline([\n",
    "    ('email_to_wordcount', EmailToWordCounterTransformer()),\n",
    "    ('wordcount_to_vector', WordCounterToVectorTransformer())\n",
    "])\n",
    "\n",
    "X_train_transformed=preprocess_pipeline.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.98125 0.98125 0.99125]\n",
      "0.9845833333333333\n"
     ]
    }
   ],
   "source": [
    "log_clf=LogisticRegression(max_iter=1000, random_state=42)\n",
    "score=cross_val_score(log_clf, X_train_transformed, y_train, cv=3)\n",
    "print(score)\n",
    "print(score.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 94.90%\n",
      "Recall: 97.89%\n"
     ]
    }
   ],
   "source": [
    "X_test_transformed=preprocess_pipeline.transform(X_test)\n",
    "\n",
    "log_clf=LogisticRegression(max_iter=1000, random_state=42)\n",
    "log_clf.fit(X_train_transformed, y_train)\n",
    "\n",
    "y_pred=log_clf.predict(X_test_transformed)\n",
    "\n",
    "print(f'Precision: {precision_score(y_test, y_pred):.2%}')\n",
    "print(f'Recall: {recall_score(y_test, y_pred):.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
